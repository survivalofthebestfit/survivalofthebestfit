<!DOCTYPE html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no"><meta name="theme-color" content="#4DA5F4"><meta name="csrf-token"><meta property="og:title" content="Survival of the Best Fit"><meta property="og:description" content="Can you hire without bias?"><meta property="og:image" content="https://www.survivalofthebestfit.com/img-website/og-image.jpg"><meta property="og:url" content="https://www.survivalofthebestfit.com"><meta name="theme-color" content="#f9f0e3"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@sotbf_"><meta name="twitter:title" content="Algorithms can be biased"><meta name="twitter:description" content="Learn about how machine learning models can inherit human bias"><meta name="twitter:image" content="https://www.survivalofthebestfit.com/img-website/sotbf-logo.jpg"><title> Survival of the Best Fit</title><link rel="favicon" href="../favicon.ico"><link rel="stylesheet" href="../vendor/bootstrap/css/bootstrap.min.css"><link rel="stylesheet" href="../vendor/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,700"><link href="https://fonts.googleapis.com/css?family=Nunito:300,400,700&amp;display=swap" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Open Sans:400,700&amp;display=swap" rel="stylesheet"><link rel="stylesheet" href="../css/styles-website.min.css"><!-- Navigation--><nav class="navbar navbar-expand-md navbar-light justify-content-between sticky-top" id="mainNav"><div class="container"><div class="collapse navbar-collapse dual-nav w-50 order-1 order-md-0" id="navbarResponsive"><ul class="navbar-nav"><li class="nav-item"><a class="nav-link pl-0 js-scroll-trigger" href="./">Home</a></li><li class="nav-item"><a class="nav-link pl-0 js-scroll-trigger" href="./about">About</a></li><li class="nav-item"><a class="nav-link pl-0 js-scroll-trigger" href="./resources">Resources</a></li></ul></div><button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target=".dual-nav" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><a class="navbar-brand mx-auto d-block text-center order-0 order-md-1 w-25" href="./">SOTBF</a><div class="navbar-collapse collapse dual-nav w-50 order-2"><a class="btn btn-info ml-auto navbar-play-button navbar-play" href="./game" role="button">Start Game</a></div></div></nav></head><body><div id="fb-root"></div>
<script async defer crossorigin="anonymous" src="https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v3.3&appId=557447817930570"></script><section class="resources-section text-center"><div class="container"><div class="row"><div class="col-lg-9 mx-auto"><h1>Understanding Algorithmic Bias</h1><img class="img-fluid img-asset" src="./img-website/table-asset.png"></div></div><div class="row table-of-contents mt-5"><div class="col-lg-4 col-md-6 col-xs-8 mx-auto"><h3 class="text-center">Table of Contents</h3><ul class="list-unstyled mt-3"><li><a href="#reading1">Part 1: Behind the Technology</a></li><li><a href="#reading2">Part 2: Fair Software</a></li><li><a href="#reading3">Part 3: Steps Forward</a></li><li><a href="#reading4">More Resources</a></li></ul></div></div><div class="row"><div class="col-lg-9 mx-auto"><div class="cute-box" id="reading1" onclick="boxClicked(&quot;#reading1&quot;)"><h2>Part 1: Behind the Technology</h2><p>You don't need to be an engineer to question how technology is affecting our lives. The goal is not for everyone to be a data scientist or machine learning engineer, though the field can certainly use <a href='https://medium.com/@AINowInstitute/gender-race-and-power-in-ai-a-playlist-2d3a44e43d3b'>more diversity</a>, but to have enough awareness to join the conversation and ask important questions.</p><iframe class="giphy-embed" src="https://giphy.com/embed/oSjA9HcU0iIXm" width="480" height="408" frameBorder="0" allowFullScreen></iframe><p><a href='https://www.youtube.com/watch?v=z-EtmaFJieY'> Machine learning (ML)</a>, is an application of artificial intelligence (AI) that deals with teaching computers how to make decisions by learning from large amounts of data. AI can be a powerful tool in helping make decisions fast, but its "automated" decision-making doesn't mean that it is always correct, objective, or fair.</p><p>When building automated decision-making systems, we should be asking: what metrics are these decisions based on, and who can we hold accountable? With machine learning, that becomes tricky, because after the program is trained on massive amounts of historical data, finding out why a certain decision was made is no longer straightforward. It's what is referred to as the <a href="https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/">Black Box problem</a>.</p><iframe class="giphy-embed" src="https://giphy.com/embed/l2JegI3GShJoyHEwo" width="480" height="408" frameBorder="0" allowFullScreen></iframe><p>Technology should help us move forward, and not repeat the mistakes of the past. If the data we use, for example, represents a history when only people of a certain race, gender, or class were privileged by the system, then what are we teaching our software? </p></div><div class="cute-box clickable" id="reading2" onclick="boxClicked(&quot;#reading2&quot;)"><h2>Part 2: Fair Software</h2><h4>Click me for more GIFs...</h4><p>This is a complicated problem with a complicated solution. And technology alone is not it.</p><iframe class="giphy-embed" src="https://giphy.com/embed/XI3CMlr4RnaXS" width="480" height="408" frameBorder="0" allowFullScreen></iframe><p>Depending on the data fed into a machine learning software, it could mean that the decisions it produces can abide by negative stereotypes, be modeled on long-standing injustice, or exclude certain groups of people. We want to make sure that technology works for everyone, and doesn't repeat historic injustices and inequalities in <a href="https://points.datasociety.net/systemic-algorithmic-harms-e00f99e72c42">systemic ways</a>. Fairness is subjective, and bias is complicated. We don't expect software engineers to solve it all - which is why we think you should be a part of the conversation. Building software isn't a purely technical challenge when it involves many social, political, and economic implications. Whether it relates to algorithmic bias, or other ways tech is affecting our lives, we need to be able to ask questions, and hold developers accountable.</p><iframe class="giphy-embed" src="https://giphy.com/embed/xBqoUR9qjWPCtUMhu9" width="480" height="408" frameBorder="0" allowFullScreen></iframe><p>Sometimes we must ask: how does this software not oppress or exclude? Is using this technology suitable at all? If a machine learning solution doesn't allow us to question its outcome, should governments and corporations be able to use automated decision-making to absolve themselves from responsibility? </p></div><div class="cute-box clickable" id="reading3" onclick="boxClicked(&quot;#reading3&quot;)"><h2>Part 3: Steps Forward</h2><h4>Click me to become a fairness expert...</h4><p>We don't have a shortcut solution to "fix" algorithmic bias, but we do have many great people working on it. We don't want to reinvent the wheel, but to encourage you to support and expand on ongoing efforts. Building ethical, equitable technology doesn't start or stop at algorithmic bias, which is why we hope the public at large becomes better informed of how technology affects our lives at large.</p><iframe src="https://giphy.com/embed/DNpWCDNx03IXlETSac" width="480" height="270" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p>We need collaboration among individuals and organizations across various fields both to take strides forward in building more just technology and to hold those developing it accountable. This is why we want issues in tech ethics to be accessible to those who may have not taken a computer science class before, but still, have a lot to add to the conversation.</p><p>There are many angles to approaching fair tech, and they include greater public awareness and advocacy, <a href='https://blog.usejournal.com/from-principles-to-action-how-do-we-implement-tech-ethics-d59aebb05ed8?sk=d01ca6f6d827b44414d34730dd920ab2'>holding tech companies accountable</a>, engaging more diverse voices, and developing more <a href='https://www.fastcompany.com/90355969/want-to-fix-big-tech-change-what-classes-are-required-for-a-computer-science-degree'>socially conscious CS education curricula</a>.</p><iframe src="https://giphy.com/embed/JUALKCjmb9SkrYrx46" width="480" height="269" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p>If you would like to learn more about algorithmic bias or the impact of technology on our societies, check out our reading list below. Then, we hope you will ask questions, voice your opinions, and support the call for technology that benefits our societies.</p></div><div class="cute-box clickable" id="reading4" onclick="boxClicked(&quot;#reading4&quot;)"><h2>More Resources</h2><h4>Click me to see where we got all this from...</h4><iframe src="https://giphy.com/embed/eCwAEs05phtK" width="480" height="480" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p>To learn more about the health of the internet, you can start with Mozilla's  <a href="https://internethealthreport.org/2019/">Internet Health 2019 Report</a></p><p>You can find more readings and research on algorithmic bias and ethical technology here in  <a href="https://github.com/survivalofthebestfit/survivalofthebestfit/wiki/Reading-List">our reading list</a>.</p></div></div></div></div></section><!-- Footer--><footer class="footer"><div class="container"><div class="row"><div class="col-xs-12 mt-4 mx-auto"><ul class="list-unstyled list-inline social text-center"><li class="list-inline-item"><a href="mailto:survivalofthebestfit@gmail.com"><i class="fas fa-envelope fa-lg"></i></a></li><li class="list-inline-item"><a href="https://github.com/survivalofthebestfit/survivalofthebestfit"><i class="fab fa-github fa-lg"></i></a></li><li class="list-inline-item"><a href="https://twitter.com/sotbf_"><i class="fab fa-twitter fa-lg"></i></a></li><li class="list-inline-item"><a href="https://medium.com/survival-of-the-best-fit"><i class="fab fa-medium fa-lg"></i></a></li></ul></div></div><row><div class="col-xs-12 my-2 text-center mx-auto"><p> &copy 2019 Copyright.<a class="ml-2" href="https://survivalofthebestfit.com">Survival of the Best Fit</a></p></div></row></div></footer><script src="./vendor/jquery/jquery.min.js"></script><script src="./vendor/bootstrap/js/bootstrap.bundle.min.js"></script><script src="./vendor/jquery-easing/jquery.easing.min.js"></script><script src="scripts-website.min.js"></script></body><script async src="https://www.googletagmanager.com/gtag/js?id=UA-138331065-1"></script><script>window.dataLayer = window.dataLayer || []
function gtag(){dataLayer.push(arguments)}
gtag('js', new Date())
gtag('config', 'UA-138331065-1')</script></html>